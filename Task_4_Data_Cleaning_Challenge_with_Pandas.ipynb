{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Importing libararies\n",
        "# ============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "pCRDQxZzo_Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD AMAZON SALES DATASET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading Amazon Sale Report.csv...\")\n",
        "df_original = pd.read_csv('/content/drive/MyDrive/Cloud Honors/Amazon Sale Report.csv')\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Shape: {df_original.shape}\")\n",
        "print(f\"Rows: {df_original.shape[0]:,}\")\n",
        "print(f\"Columns: {df_original.shape[1]}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df_original.head())\n",
        "print(f\"\\nColumn names and types:\")\n",
        "print(df_original.dtypes)\n",
        "print(f\"\\nDataset Info:\")\n",
        "df_original.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9Qmhz8epUwh",
        "outputId": "c92630c1-1b70-499d-a78b-a26a2e222863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Amazon Sale Report.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-588262479.py:6: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_original = pd.read_csv('/content/drive/MyDrive/Cloud Honors/Amazon Sale Report.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset loaded successfully!\n",
            "Shape: (128975, 24)\n",
            "Rows: 128,975\n",
            "Columns: 24\n",
            "\n",
            "First few rows:\n",
            "   index             Order ID      Date                        Status  \\\n",
            "0      0  405-8078784-5731545  04-30-22                     Cancelled   \n",
            "1      1  171-9198151-1101146  04-30-22  Shipped - Delivered to Buyer   \n",
            "2      2  404-0687676-7273146  04-30-22                       Shipped   \n",
            "3      3  403-9615377-8133951  04-30-22                     Cancelled   \n",
            "4      4  407-1069790-7240320  04-30-22                       Shipped   \n",
            "\n",
            "  Fulfilment Sales Channel  ship-service-level    Style              SKU  \\\n",
            "0   Merchant      Amazon.in           Standard   SET389   SET389-KR-NP-S   \n",
            "1   Merchant      Amazon.in           Standard  JNE3781  JNE3781-KR-XXXL   \n",
            "2     Amazon      Amazon.in          Expedited  JNE3371    JNE3371-KR-XL   \n",
            "3   Merchant      Amazon.in           Standard    J0341       J0341-DR-L   \n",
            "4     Amazon      Amazon.in          Expedited  JNE3671  JNE3671-TU-XXXL   \n",
            "\n",
            "        Category  ... currency  Amount    ship-city   ship-state  \\\n",
            "0            Set  ...      INR  647.62       MUMBAI  MAHARASHTRA   \n",
            "1          kurta  ...      INR  406.00    BENGALURU    KARNATAKA   \n",
            "2          kurta  ...      INR  329.00  NAVI MUMBAI  MAHARASHTRA   \n",
            "3  Western Dress  ...      INR  753.33   PUDUCHERRY   PUDUCHERRY   \n",
            "4            Top  ...      INR  574.00      CHENNAI   TAMIL NADU   \n",
            "\n",
            "  ship-postal-code  ship-country  \\\n",
            "0         400081.0            IN   \n",
            "1         560085.0            IN   \n",
            "2         410210.0            IN   \n",
            "3         605008.0            IN   \n",
            "4         600073.0            IN   \n",
            "\n",
            "                                       promotion-ids    B2B  fulfilled-by  \\\n",
            "0                                                NaN  False     Easy Ship   \n",
            "1  Amazon PLCC Free-Financing Universal Merchant ...  False     Easy Ship   \n",
            "2       IN Core Free Shipping 2015/04/08 23-48-5-108   True           NaN   \n",
            "3                                                NaN  False     Easy Ship   \n",
            "4                                                NaN  False           NaN   \n",
            "\n",
            "  Unnamed: 22  \n",
            "0         NaN  \n",
            "1         NaN  \n",
            "2         NaN  \n",
            "3         NaN  \n",
            "4         NaN  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "\n",
            "Column names and types:\n",
            "index                   int64\n",
            "Order ID               object\n",
            "Date                   object\n",
            "Status                 object\n",
            "Fulfilment             object\n",
            "Sales Channel          object\n",
            "ship-service-level     object\n",
            "Style                  object\n",
            "SKU                    object\n",
            "Category               object\n",
            "Size                   object\n",
            "ASIN                   object\n",
            "Courier Status         object\n",
            "Qty                     int64\n",
            "currency               object\n",
            "Amount                float64\n",
            "ship-city              object\n",
            "ship-state             object\n",
            "ship-postal-code      float64\n",
            "ship-country           object\n",
            "promotion-ids          object\n",
            "B2B                      bool\n",
            "fulfilled-by           object\n",
            "Unnamed: 22            object\n",
            "dtype: object\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 128975 entries, 0 to 128974\n",
            "Data columns (total 24 columns):\n",
            " #   Column              Non-Null Count   Dtype  \n",
            "---  ------              --------------   -----  \n",
            " 0   index               128975 non-null  int64  \n",
            " 1   Order ID            128975 non-null  object \n",
            " 2   Date                128975 non-null  object \n",
            " 3   Status              128975 non-null  object \n",
            " 4   Fulfilment          128975 non-null  object \n",
            " 5   Sales Channel       128975 non-null  object \n",
            " 6   ship-service-level  128975 non-null  object \n",
            " 7   Style               128975 non-null  object \n",
            " 8   SKU                 128975 non-null  object \n",
            " 9   Category            128975 non-null  object \n",
            " 10  Size                128975 non-null  object \n",
            " 11  ASIN                128975 non-null  object \n",
            " 12  Courier Status      122103 non-null  object \n",
            " 13  Qty                 128975 non-null  int64  \n",
            " 14  currency            121180 non-null  object \n",
            " 15  Amount              121180 non-null  float64\n",
            " 16  ship-city           128942 non-null  object \n",
            " 17  ship-state          128942 non-null  object \n",
            " 18  ship-postal-code    128942 non-null  float64\n",
            " 19  ship-country        128942 non-null  object \n",
            " 20  promotion-ids       79822 non-null   object \n",
            " 21  B2B                 128975 non-null  bool   \n",
            " 22  fulfilled-by        39277 non-null   object \n",
            " 23  Unnamed: 22         79925 non-null   object \n",
            "dtypes: bool(1), float64(2), int64(2), object(19)\n",
            "memory usage: 22.8+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# ANALYZE DATA QUALITY BEFORE CLEANING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA QUALITY ANALYSIS - BEFORE CLEANING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_check = df_original.copy()\n",
        "print(f\"\\nDataset Shape: {df_check.shape}\")\n",
        "print(f\"Total Records: {len(df_check):,}\")\n",
        "print(f\"Total Columns: {len(df_check.columns)}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"MISSING VALUES ANALYSIS:\")\n",
        "print(\"-\"*80)\n",
        "missing_values = df_check.isnull().sum()\n",
        "print(missing_values)\n",
        "print(f\"\\nTotal Missing Values: {missing_values.sum():,}\")\n",
        "print(f\"Percentage: {(missing_values.sum() / (len(df_check) * len(df_check.columns)) * 100):.2f}%\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"DUPLICATE ROWS ANALYSIS:\")\n",
        "print(\"-\"*80)\n",
        "duplicates = df_check.duplicated().sum()\n",
        "print(f\"Total Duplicate Rows: {duplicates:,}\")\n",
        "\n",
        "print(f\"\\n\" + \"-\"*80)\n",
        "print(\"DATA TYPES:\")\n",
        "print(\"-\"*80)\n",
        "print(df_check.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktBOEIa7plKt",
        "outputId": "4dc35da0-678c-4799-e822-deeb696fb713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DATA QUALITY ANALYSIS - BEFORE CLEANING\n",
            "================================================================================\n",
            "\n",
            "Dataset Shape: (128975, 24)\n",
            "Total Records: 128,975\n",
            "Total Columns: 24\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MISSING VALUES ANALYSIS:\n",
            "--------------------------------------------------------------------------------\n",
            "index                     0\n",
            "Order ID                  0\n",
            "Date                      0\n",
            "Status                    0\n",
            "Fulfilment                0\n",
            "Sales Channel             0\n",
            "ship-service-level        0\n",
            "Style                     0\n",
            "SKU                       0\n",
            "Category                  0\n",
            "Size                      0\n",
            "ASIN                      0\n",
            "Courier Status         6872\n",
            "Qty                       0\n",
            "currency               7795\n",
            "Amount                 7795\n",
            "ship-city                33\n",
            "ship-state               33\n",
            "ship-postal-code         33\n",
            "ship-country             33\n",
            "promotion-ids         49153\n",
            "B2B                       0\n",
            "fulfilled-by          89698\n",
            "Unnamed: 22           49050\n",
            "dtype: int64\n",
            "\n",
            "Total Missing Values: 210,495\n",
            "Percentage: 6.80%\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DUPLICATE ROWS ANALYSIS:\n",
            "--------------------------------------------------------------------------------\n",
            "Total Duplicate Rows: 0\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATA TYPES:\n",
            "--------------------------------------------------------------------------------\n",
            "index                   int64\n",
            "Order ID               object\n",
            "Date                   object\n",
            "Status                 object\n",
            "Fulfilment             object\n",
            "Sales Channel          object\n",
            "ship-service-level     object\n",
            "Style                  object\n",
            "SKU                    object\n",
            "Category               object\n",
            "Size                   object\n",
            "ASIN                   object\n",
            "Courier Status         object\n",
            "Qty                     int64\n",
            "currency               object\n",
            "Amount                float64\n",
            "ship-city              object\n",
            "ship-state             object\n",
            "ship-postal-code      float64\n",
            "ship-country           object\n",
            "promotion-ids          object\n",
            "B2B                      bool\n",
            "fulfilled-by           object\n",
            "Unnamed: 22            object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# HANDLE MISSING VALUES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nStep 1: Handling Missing Values...\")\n",
        "df_cleaned = df_original.copy()\n",
        "\n",
        "# For numeric columns, fill with median\n",
        "for col in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    if df_cleaned[col].isnull().sum() > 0:\n",
        "        df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
        "\n",
        "# For object (string) columns, fill with 'Unknown'\n",
        "for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
        "    if df_cleaned[col].isnull().sum() > 0:\n",
        "        df_cleaned[col].fillna('Unknown', inplace=True)\n",
        "\n",
        "print(f\"Missing values after handling: {df_cleaned.isnull().sum().sum()}\")\n",
        "print(\"✓ All missing values handled!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnv0fNcBptdL",
        "outputId": "e7d45927-3531-4f4f-b88b-67859b9bd23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 1: Handling Missing Values...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1478119931.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
            "/tmp/ipython-input-1478119931.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_cleaned[col].fillna('Unknown', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after handling: 0\n",
            "✓ All missing values handled!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# REMOVE DUPLICATE ROWS\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 2: Removing Duplicate Rows...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Count duplicates before removal\n",
        "duplicate_rows_before = df_cleaned.duplicated().sum()\n",
        "print(f\"\\nDuplicate Rows Before Removal: {duplicate_rows_before}\")\n",
        "\n",
        "# Remove duplicate rows\n",
        "df_cleaned = df_cleaned.drop_duplicates()\n",
        "\n",
        "# Count duplicates after removal\n",
        "duplicate_rows_after = df_cleaned.duplicated().sum()\n",
        "print(f\"Duplicate Rows After Removal: {duplicate_rows_after}\")\n",
        "print(f\"✓ Removed {duplicate_rows_before - duplicate_rows_after} duplicate row(s)\")\n",
        "print(f\"\\nDataset Shape After Removing Duplicates: {df_cleaned.shape}\")\n",
        "print(f\"Total Rows Remaining: {len(df_cleaned):,} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sevn0Iz2qHOe",
        "outputId": "2b60dc99-cb74-4e2e-b5ec-d323fae8d503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Removing Duplicate Rows...\n",
            "================================================================================\n",
            "\n",
            "Duplicate Rows Before Removal: 0\n",
            "Duplicate Rows After Removal: 0\n",
            "✓ Removed 0 duplicate row(s)\n",
            "\n",
            "Dataset Shape After Removing Duplicates: (128975, 24)\n",
            "Total Rows Remaining: 128,975 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# FIX INCONSISTENT FORMATS (TEXT NORMALIZATION)\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 3: Fixing Inconsistent Formats...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Apply text normalization to all object (string) columns\n",
        "object_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(f\"\\nApplying format normalization to {len(object_columns)} text columns:\")\n",
        "for col in object_columns:\n",
        "    # Strip whitespace and convert to title case for consistency\n",
        "    df_cleaned[col] = df_cleaned[col].str.strip().str.title()\n",
        "    print(f\"  ✓ Normalized: {col}\")\n",
        "\n",
        "print(f\"\\n✓ All text columns have been normalized\")\n",
        "print(f\"\\nNormalization applied:\")\n",
        "print(f\"  - Removed leading/trailing whitespace\")\n",
        "print(f\"  - Converted to Title Case for consistency\")\n",
        "print(f\"\\nDataset shape remains: {df_cleaned.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpKPnkDZqg-x",
        "outputId": "335ac6da-b9c2-4ae4-dbe3-f331457f7f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Fixing Inconsistent Formats...\n",
            "================================================================================\n",
            "\n",
            "Applying format normalization to 19 text columns:\n",
            "  ✓ Normalized: Order ID\n",
            "  ✓ Normalized: Date\n",
            "  ✓ Normalized: Status\n",
            "  ✓ Normalized: Fulfilment\n",
            "  ✓ Normalized: Sales Channel \n",
            "  ✓ Normalized: ship-service-level\n",
            "  ✓ Normalized: Style\n",
            "  ✓ Normalized: SKU\n",
            "  ✓ Normalized: Category\n",
            "  ✓ Normalized: Size\n",
            "  ✓ Normalized: ASIN\n",
            "  ✓ Normalized: Courier Status\n",
            "  ✓ Normalized: currency\n",
            "  ✓ Normalized: ship-city\n",
            "  ✓ Normalized: ship-state\n",
            "  ✓ Normalized: ship-country\n",
            "  ✓ Normalized: promotion-ids\n",
            "  ✓ Normalized: fulfilled-by\n",
            "  ✓ Normalized: Unnamed: 22\n",
            "\n",
            "✓ All text columns have been normalized\n",
            "\n",
            "Normalization applied:\n",
            "  - Removed leading/trailing whitespace\n",
            "  - Converted to Title Case for consistency\n",
            "\n",
            "Dataset shape remains: (128975, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# RENAME COLUMNS TO STANDARD FORMAT\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 4: Renaming Columns to Standard Format...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a copy of original column names\n",
        "original_columns = df_cleaned.columns.tolist()\n",
        "\n",
        "# Convert column names to snake_case\n",
        "def to_snake_case(name):\n",
        "    # Replace spaces and hyphens with underscores\n",
        "    name = name.replace(' ', '_').replace('-', '_')\n",
        "    # Convert to lowercase\n",
        "    name = name.lower()\n",
        "    # Remove multiple consecutive underscores\n",
        "    while '__' in name:\n",
        "        name = name.replace('__', '_')\n",
        "    return name\n",
        "\n",
        "# Apply the conversion\n",
        "new_columns = {col: to_snake_case(col) for col in original_columns}\n",
        "df_cleaned.rename(columns=new_columns, inplace=True)\n",
        "\n",
        "print(f\"\\nColumn Renaming Summary:\")\n",
        "print(\"-\" * 80)\n",
        "for old, new in list(new_columns.items())[:10]:\n",
        "    if old != new:\n",
        "        print(f\"  {old:30} --> {new}\")\n",
        "if len(new_columns) > 10:\n",
        "    print(f\"  ... and {len(new_columns) - 10} more columns\")\n",
        "\n",
        "print(f\"\\n✓ All {len(original_columns)} columns renamed to snake_case\")\n",
        "print(f\"\\nNew Column Names:\")\n",
        "print(df_cleaned.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvXrQx6rqruL",
        "outputId": "b5237b73-d754-481b-8d4a-9ac482ce696e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Renaming Columns to Standard Format...\n",
            "================================================================================\n",
            "\n",
            "Column Renaming Summary:\n",
            "--------------------------------------------------------------------------------\n",
            "  Order ID                       --> order_id\n",
            "  Date                           --> date\n",
            "  Status                         --> status\n",
            "  Fulfilment                     --> fulfilment\n",
            "  Sales Channel                  --> sales_channel_\n",
            "  ship-service-level             --> ship_service_level\n",
            "  Style                          --> style\n",
            "  SKU                            --> sku\n",
            "  Category                       --> category\n",
            "  ... and 14 more columns\n",
            "\n",
            "✓ All 24 columns renamed to snake_case\n",
            "\n",
            "New Column Names:\n",
            "['index', 'order_id', 'date', 'status', 'fulfilment', 'sales_channel_', 'ship_service_level', 'style', 'sku', 'category', 'size', 'asin', 'courier_status', 'qty', 'currency', 'amount', 'ship_city', 'ship_state', 'ship_postal_code', 'ship_country', 'promotion_ids', 'b2b', 'fulfilled_by', 'unnamed:_22']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# FILTER AND SUBSET DATA\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 5: Filtering and Subsetting Data...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display initial dataset size\n",
        "print(f\"\\nOriginal Dataset Size: {len(df_cleaned):,} rows\")\n",
        "\n",
        "# Example filters:\n",
        "# 1. Filter by Status (keep only specific statuses)\n",
        "valid_statuses = df_cleaned['status'].value_counts().head(5).index.tolist()\n",
        "print(f\"\\nTop 5 Status values: {valid_statuses}\")\n",
        "\n",
        "# 2. Filter for records with valid status\n",
        "df_filtered = df_cleaned[df_cleaned['status'].isin(valid_statuses)].copy()\n",
        "rows_removed = len(df_cleaned) - len(df_filtered)\n",
        "print(f\"\\nFilter Applied: Keep only top 5 statuses\")\n",
        "print(f\"  Rows removed: {rows_removed:,}\")\n",
        "print(f\"  Rows remaining: {len(df_filtered):,}\")\n",
        "\n",
        "# 3. Filter by non-null amounts (ensure valid transaction amounts)\n",
        "df_filtered = df_filtered[df_filtered['amount'].notna()].copy()\n",
        "print(f\"\\nFilter Applied: Remove rows with null Amount\")\n",
        "print(f\"  Rows remaining: {len(df_filtered):,}\")\n",
        "\n",
        "# 4. Subset specific columns for analysis\n",
        "analysis_columns = ['order_id', 'date', 'status', 'amount', 'qty']\n",
        "available_cols = [col for col in analysis_columns if col in df_filtered.columns]\n",
        "df_subset = df_filtered[available_cols].copy()\n",
        "\n",
        "print(f\"\\nColumn Subsetting Applied:\")\n",
        "print(f\"  Selected columns: {available_cols}\")\n",
        "print(f\"  Subset shape: {df_subset.shape}\")\n",
        "\n",
        "print(f\"\\n✓ Filtered and subset dataset ready for analysis\")\n",
        "print(f\"  Original rows: {len(df_cleaned):,}\")\n",
        "print(f\"  Final rows: {len(df_filtered):,}\")\n",
        "print(f\"  Reduction: {((len(df_cleaned) - len(df_filtered)) / len(df_cleaned) * 100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOV5h23Zq5TN",
        "outputId": "e5210cf3-75bd-4685-c533-686a73ed4333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 5: Filtering and Subsetting Data...\n",
            "================================================================================\n",
            "\n",
            "Original Dataset Size: 128,975 rows\n",
            "\n",
            "Top 5 Status values: ['Shipped', 'Shipped - Delivered To Buyer', 'Cancelled', 'Shipped - Returned To Seller', 'Shipped - Picked Up']\n",
            "\n",
            "Filter Applied: Keep only top 5 statuses\n",
            "  Rows removed: 1,144\n",
            "  Rows remaining: 127,831\n",
            "\n",
            "Filter Applied: Remove rows with null Amount\n",
            "  Rows remaining: 127,831\n",
            "\n",
            "Column Subsetting Applied:\n",
            "  Selected columns: ['order_id', 'date', 'status', 'amount', 'qty']\n",
            "  Subset shape: (127831, 5)\n",
            "\n",
            "✓ Filtered and subset dataset ready for analysis\n",
            "  Original rows: 128,975\n",
            "  Final rows: 127,831\n",
            "  Reduction: 0.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# GROUP AND AGGREGATE DATA\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 6: Group and Aggregate Data...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Group by Status and calculate aggregations\n",
        "print(\"\\n1. Aggregation by Status:\")\n",
        "print(\"-\" * 80)\n",
        "status_agg = df_cleaned.groupby('status').agg({\n",
        "    'amount': ['sum', 'mean', 'count'],\n",
        "    'qty': 'sum'\n",
        "}).round(2)\n",
        "status_agg.columns = ['Total_Amount', 'Average_Amount', 'Order_Count', 'Total_Qty']\n",
        "print(status_agg.head())\n",
        "\n",
        "# Group by Sales Channel\n",
        "print(\"\\n2. Aggregation by Sales Channel:\")\n",
        "print(\"-\" * 80)\n",
        "channel_agg = df_cleaned.groupby('sales_channel_').agg({\n",
        "    'amount': ['sum', 'mean', 'count']\n",
        "}).round(2)\n",
        "channel_agg.columns = ['Total_Amount', 'Average_Amount', 'Order_Count']\n",
        "print(channel_agg)\n",
        "\n",
        "# Calculate summary statistics\n",
        "print(\"\\n3. Overall Summary Statistics:\")\n",
        "print(\"-\" * 80)\n",
        "summary_stats = {\n",
        "    'Total Orders': len(df_cleaned),\n",
        "    'Total Amount': f\"${df_cleaned['amount'].sum():,.2f}\",\n",
        "    'Average Amount': f\"${df_cleaned['amount'].mean():,.2f}\",\n",
        "    'Total Quantity': int(df_cleaned['qty'].sum()),\n",
        "    'Unique Status': df_cleaned['status'].nunique(),\n",
        "    'Unique Channels': df_cleaned['sales_channel_'].nunique()\n",
        "}\n",
        "for key, value in summary_stats.items():\n",
        "    print(f\"  {key:.<35} {value}\")\n",
        "\n",
        "print(f\"\\n✓ Grouping and aggregation operations completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_AJv4tXrHIH",
        "outputId": "5a60db28-d320-4600-e610-de1c1e639210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Group and Aggregate Data...\n",
            "================================================================================\n",
            "\n",
            "1. Aggregation by Status:\n",
            "--------------------------------------------------------------------------------\n",
            "                               Total_Amount  Average_Amount  Order_Count  \\\n",
            "status                                                                     \n",
            "Cancelled                        11496714.3          627.14        18332   \n",
            "Pending                            431481.0          655.75          658   \n",
            "Pending - Waiting For Pick Up      192138.0          683.77          281   \n",
            "Shipped                          50450095.0          648.43        77804   \n",
            "Shipped - Damaged                    1136.0         1136.00            1   \n",
            "\n",
            "                               Total_Qty  \n",
            "status                                    \n",
            "Cancelled                           5657  \n",
            "Pending                              657  \n",
            "Pending - Waiting For Pick Up        283  \n",
            "Shipped                            78009  \n",
            "Shipped - Damaged                      1  \n",
            "\n",
            "2. Aggregation by Sales Channel:\n",
            "--------------------------------------------------------------------------------\n",
            "                Total_Amount  Average_Amount  Order_Count\n",
            "sales_channel_                                           \n",
            "Amazon.In         83233633.3          645.97       128851\n",
            "Non-Amazon           75020.0          605.00          124\n",
            "\n",
            "3. Overall Summary Statistics:\n",
            "--------------------------------------------------------------------------------\n",
            "  Total Orders....................... 128975\n",
            "  Total Amount....................... $83,308,653.30\n",
            "  Average Amount..................... $645.93\n",
            "  Total Quantity..................... 116649\n",
            "  Unique Status...................... 13\n",
            "  Unique Channels.................... 2\n",
            "\n",
            "✓ Grouping and aggregation operations completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CONVERT COLUMN DATA TYPES\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 7: Converting Data Types...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display original data types\n",
        "print(\"\\nOriginal Data Types:\")\n",
        "print(\"-\" * 80)\n",
        "original_dtypes = df_cleaned.dtypes\n",
        "print(original_dtypes)\n",
        "\n",
        "# Define type conversions\n",
        "print(\"\\nApplying Data Type Conversions:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Convert numeric columns\n",
        "for col in ['qty']:\n",
        "    if col in df_cleaned.columns:\n",
        "        df_cleaned[col] = df_cleaned[col].astype('int64')\n",
        "        print(f\"  ✓ Converted '{col}' to int64\")\n",
        "\n",
        "# Convert amount to float64 if not already\n",
        "for col in ['amount', 'ship_postal_code']:\n",
        "    if col in df_cleaned.columns and df_cleaned[col].dtype != 'float64':\n",
        "        try:\n",
        "            df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "            print(f\"  ✓ Converted '{col}' to float64\")\n",
        "        except:\n",
        "            print(f\"  - Could not convert '{col}'\")\n",
        "\n",
        "# Convert date column to datetime\n",
        "if 'date' in df_cleaned.columns:\n",
        "    df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce')\n",
        "    print(f\"  ✓ Converted 'date' to datetime64\")\n",
        "\n",
        "print(\"\\nFinal Data Types:\")\n",
        "print(\"-\" * 80)\n",
        "final_dtypes = df_cleaned.dtypes\n",
        "print(final_dtypes)\n",
        "\n",
        "print(f\"\\n✓ Data type conversions completed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQrs7ChGrUbL",
        "outputId": "65184123-973d-403a-a49a-989325af6c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7: Converting Data Types...\n",
            "================================================================================\n",
            "\n",
            "Original Data Types:\n",
            "--------------------------------------------------------------------------------\n",
            "index                   int64\n",
            "order_id               object\n",
            "date                   object\n",
            "status                 object\n",
            "fulfilment             object\n",
            "sales_channel_         object\n",
            "ship_service_level     object\n",
            "style                  object\n",
            "sku                    object\n",
            "category               object\n",
            "size                   object\n",
            "asin                   object\n",
            "courier_status         object\n",
            "qty                     int64\n",
            "currency               object\n",
            "amount                float64\n",
            "ship_city              object\n",
            "ship_state             object\n",
            "ship_postal_code      float64\n",
            "ship_country           object\n",
            "promotion_ids          object\n",
            "b2b                      bool\n",
            "fulfilled_by           object\n",
            "unnamed:_22            object\n",
            "dtype: object\n",
            "\n",
            "Applying Data Type Conversions:\n",
            "--------------------------------------------------------------------------------\n",
            "  ✓ Converted 'qty' to int64\n",
            "  ✓ Converted 'date' to datetime64\n",
            "\n",
            "Final Data Types:\n",
            "--------------------------------------------------------------------------------\n",
            "index                          int64\n",
            "order_id                      object\n",
            "date                  datetime64[ns]\n",
            "status                        object\n",
            "fulfilment                    object\n",
            "sales_channel_                object\n",
            "ship_service_level            object\n",
            "style                         object\n",
            "sku                           object\n",
            "category                      object\n",
            "size                          object\n",
            "asin                          object\n",
            "courier_status                object\n",
            "qty                            int64\n",
            "currency                      object\n",
            "amount                       float64\n",
            "ship_city                     object\n",
            "ship_state                    object\n",
            "ship_postal_code             float64\n",
            "ship_country                  object\n",
            "promotion_ids                 object\n",
            "b2b                             bool\n",
            "fulfilled_by                  object\n",
            "unnamed:_22                   object\n",
            "dtype: object\n",
            "\n",
            "✓ Data type conversions completed successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2781431967.py:35: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_cleaned['date'] = pd.to_datetime(df_cleaned['date'], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# FINAL ANALYSIS AND EXPORT CLEANED DATASET\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\nStep 8: Final Data Quality Analysis and Export...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Final data quality check (after all cleaning)\n",
        "print(\"\\nFINAL DATA QUALITY REPORT (AFTER CLEANING):\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Dataset Shape: {df_cleaned.shape}\")\n",
        "print(f\"Total Rows: {len(df_cleaned):,}\")\n",
        "print(f\"Total Columns: {len(df_cleaned.columns)}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df_cleaned.isnull().sum()\n",
        "if missing_values.sum() > 0:\n",
        "    print(f\"\\nMissing Values Found:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "else:\n",
        "    print(f\"\\n✓ No missing values detected\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_count = df_cleaned.duplicated().sum()\n",
        "print(f\"✓ Duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Memory usage\n",
        "print(f\"\\nMemory Usage: {df_cleaned.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Export cleaned dataset to CSV\n",
        "print(\"\\nExporting Cleaned Dataset...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = 'Cleaned_Dataset.csv'\n",
        "df_cleaned.to_csv(csv_filename, index=False)\n",
        "print(f\"✓ Cleaned dataset exported to: {csv_filename}\")\n",
        "print(f\"  File size: {len(df_cleaned):,} rows x {len(df_cleaned.columns)} columns\")\n",
        "\n",
        "# Verification\n",
        "print(\"\\nVerification of Exported File:\")\n",
        "print(\"-\" * 80)\n",
        "df_verify = pd.read_csv(csv_filename, nrows=5)\n",
        "print(f\"✓ Successfully read CSV file\")\n",
        "print(f\"  First 5 rows loaded: {len(df_verify)} rows\")\n",
        "print(f\"  Columns: {len(df_verify.columns)}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(f\"DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"=\" * 80)\n",
        "print(f\"✓ All 8 cleaning operations completed\")\n",
        "print(f\"✓ Cleaned dataset exported as: {csv_filename}\")\n",
        "print(f\"✓ Ready for report generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgzEa4Y7rdw7",
        "outputId": "6a0733d0-d831-4444-d61a-1803b3521ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8: Final Data Quality Analysis and Export...\n",
            "================================================================================\n",
            "\n",
            "FINAL DATA QUALITY REPORT (AFTER CLEANING):\n",
            "--------------------------------------------------------------------------------\n",
            "Dataset Shape: (128975, 24)\n",
            "Total Rows: 128,975\n",
            "Total Columns: 24\n",
            "\n",
            "Missing Values Found:\n",
            "unnamed:_22    79925\n",
            "dtype: int64\n",
            "✓ Duplicate rows: 0\n",
            "\n",
            "Memory Usage: 171.99 MB\n",
            "\n",
            "Exporting Cleaned Dataset...\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Cleaned dataset exported to: Cleaned_Dataset.csv\n",
            "  File size: 128,975 rows x 24 columns\n",
            "\n",
            "Verification of Exported File:\n",
            "--------------------------------------------------------------------------------\n",
            "✓ Successfully read CSV file\n",
            "  First 5 rows loaded: 5 rows\n",
            "  Columns: 24\n",
            "\n",
            "================================================================================\n",
            "DATA CLEANING COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "✓ All 8 cleaning operations completed\n",
            "✓ Cleaned dataset exported as: Cleaned_Dataset.csv\n",
            "✓ Ready for report generation\n"
          ]
        }
      ]
    }
  ]
}